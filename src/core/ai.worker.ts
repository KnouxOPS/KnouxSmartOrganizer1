// src/core/ai.worker.ts

import { WorkerMessage, WorkerResponse } from "@/types/knoux-x2";

// Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ© Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠØŒ ÙŠØªÙ… ØªÙ‡ÙŠØ¦ØªÙ‡Ø§ Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© Ø¹Ù†Ø¯ Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¹Ø§Ù…Ù„.
// Ù‡Ø°Ù‡ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙƒØ¨ÙŠØ±Ø© ÙˆÙŠØ¬Ø¨ ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø· Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡.
let featureExtractor: any = null;
let visionModel: any = null;
let isInitialized = false;

/**
 * ØªÙ‡ÙŠØ¦Ø© Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¨Ø´ÙƒÙ„ ØºÙŠØ± Ù…ØªØ²Ø§Ù…Ù†.
 * ÙŠØªÙ… Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ù‡Ø°Ù‡ Ø§Ù„ÙˆØ¸ÙŠÙØ© Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© Ø¹Ù†Ø¯ Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¹Ø§Ù…Ù„.
 */
const initializeModels = async () => {
  try {
    console.log("ğŸ§  AI Worker: Initializing advanced models...");

    // Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ­Ù…ÙŠÙ„ Ù†Ù…Ø§Ø°Ø¬ Transformers Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
    try {
      const { pipeline } = await import("@xenova/transformers");

      // ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ CLIP Ù„Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠØ©
      console.log("ğŸ“¸ Loading CLIP vision model...");
      featureExtractor = await pipeline(
        "zero-shot-image-classification",
        "Xenova/clip-vit-base-patch32",
      );

      // ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙˆØµÙ Ø§Ù„ØµÙˆØ±
      console.log("ğŸ“ Loading image captioning model...");
      visionModel = await pipeline(
        "image-to-text",
        "Xenova/vit-gpt2-image-captioning",
      );

      isInitialized = true;
      console.log("âœ… AI Worker: Advanced models loaded successfully.");
    } catch (error) {
      console.warn(
        "âš ï¸ Advanced models failed, falling back to simple analysis",
      );
      isInitialized = true; // Ø§Ù„Ø³Ù…Ø§Ø­ Ø¨Ø§Ù„Ø§Ø³ØªÙ…Ø±Ø§Ø± Ù…Ø¹ ØªØ­Ù„ÙŠÙ„ Ù…Ø¨Ø³Ø·
    }
  } catch (e: any) {
    console.error("âŒ AI Worker: Failed to load AI models:", e);
    isInitialized = true; // Ø§Ù„Ø³Ù…Ø§Ø­ Ø¨Ø§Ù„Ø§Ø³ØªÙ…Ø±Ø§Ø± Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø£Ø³Ø§Ø³ÙŠØ©
  }
};

/**
 * Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠØ© Ù…Ù† Ø§Ù„ØµÙˆØ±Ø©
 */
const extractEmbeddings = async (
  imageBitmap: ImageBitmap,
): Promise<number[]> => {
  if (featureExtractor) {
    try {
      // Ø§Ø³ØªØ®Ø¯Ø§Ù… CLIP Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠØ©
      const canvas = new OffscreenCanvas(224, 224);
      const ctx = canvas.getContext("2d")!;
      ctx.drawImage(imageBitmap, 0, 0, 224, 224);

      // Ù…Ø­Ø§ÙƒØ§Ø© Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª (ÙÙŠ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ Ø³ÙŠÙƒÙˆÙ† Ø£ÙƒØ«Ø± ØªØ¹Ù‚ÙŠØ¯Ø§Ù‹)
      const imageData = ctx.getImageData(0, 0, 224, 224);
      const features = [];

      // ØªØ­Ù„ÙŠÙ„ Ù…Ø¨Ø³Ø· Ù„Ù„Ø£Ù„ÙˆØ§Ù† ÙˆØ§Ù„Ù†Ø³Ù‚
      for (let i = 0; i < imageData.data.length; i += 400) {
        const r = imageData.data[i] / 255;
        const g = imageData.data[i + 1] / 255;
        const b = imageData.data[i + 2] / 255;
        features.push((r + g + b) / 3);
      }

      // Ø¥Ø¶Ø§ÙØ© Ù…ÙŠØ²Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©
      const avgBrightness =
        features.reduce((sum, val) => sum + val, 0) / features.length;
      const contrast = Math.sqrt(
        features.reduce(
          (sum, val) => sum + Math.pow(val - avgBrightness, 2),
          0,
        ) / features.length,
      );

      return [
        avgBrightness,
        contrast,
        ...features.slice(0, 510), // 512 Ø¨ÙØ¹Ø¯ Ø¥Ø¬Ù…Ø§Ù„ÙŠ
      ].slice(0, 512);
    } catch (error) {
      console.warn("Failed to extract advanced embeddings, using fallback");
    }
  }

  // Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨Ø¯ÙŠÙ„Ø© Ù…Ø¨Ø³Ø·Ø©
  return generateSimpleEmbeddings(imageBitmap);
};

/**
 * ØªÙˆÙ„ÙŠØ¯ ØªØ¶Ù…ÙŠÙ†Ø§Øª Ù…Ø¨Ø³Ø·Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
 */
const generateSimpleEmbeddings = (imageBitmap: ImageBitmap): number[] => {
  const canvas = new OffscreenCanvas(64, 64);
  const ctx = canvas.getContext("2d")!;
  ctx.drawImage(imageBitmap, 0, 0, 64, 64);

  const imageData = ctx.getImageData(0, 0, 64, 64);
  const features = [];

  // ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ù„ÙˆØ§Ù† ÙˆØ§Ù„Ø£Ù†Ù…Ø§Ø·
  for (let i = 0; i < imageData.data.length; i += 16) {
    const r = imageData.data[i] / 255;
    const g = imageData.data[i + 1] / 255;
    const b = imageData.data[i + 2] / 255;

    features.push(r, g, b);
  }

  // Ø­Ø³Ø§Ø¨ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø¥Ø¶Ø§ÙÙŠØ©
  const avgR =
    features.filter((_, i) => i % 3 === 0).reduce((a, b) => a + b, 0) /
    (features.length / 3);
  const avgG =
    features.filter((_, i) => i % 3 === 1).reduce((a, b) => a + b, 0) /
    (features.length / 3);
  const avgB =
    features.filter((_, i) => i % 3 === 2).reduce((a, b) => a + b, 0) /
    (features.length / 3);

  // Ù†Ø³Ø¨Ø© Ø§Ù„Ø¹Ø±Ø¶ Ø¥Ù„Ù‰ Ø§Ù„Ø§Ø±ØªÙØ§Ø¹ (Ù…Ø­Ø§ÙƒØ§Ø©)
  const aspectRatio = imageBitmap.width / imageBitmap.height;

  return [
    avgR,
    avgG,
    avgB,
    aspectRatio,
    ...features.slice(0, 508), // Ø¥Ø¬Ù…Ø§Ù„ÙŠ 512 Ø¨ÙØ¹Ø¯
  ].slice(0, 512);
};

/**
 * ØªØµÙ†ÙŠÙ Ø§Ù„ØµÙˆØ±Ø© ÙˆØªÙˆÙ„ÙŠØ¯ Ø§Ù„ÙˆØµÙ
 */
const classifyAndDescribe = async (
  imageBitmap: ImageBitmap,
): Promise<{ classification: string; description: string; tags: string[] }> => {
  if (visionModel && featureExtractor) {
    try {
      // ØªØµÙ†ÙŠÙ Ù…ØªÙ‚Ø¯Ù…
      const classification = await featureExtractor(imageBitmap, [
        "people",
        "nature",
        "food",
        "vehicle",
        "building",
        "animal",
        "document",
        "art",
      ]);

      // ØªÙˆÙ„ÙŠØ¯ ÙˆØµÙ
      const description = await visionModel(imageBitmap);

      return {
        classification: classification[0]?.label || "Ø¹Ø§Ù…",
        description: description[0]?.generated_text || "ØµÙˆØ±Ø© Ø¬Ù…ÙŠÙ„Ø©",
        tags: classification.slice(0, 3).map((c: any) => c.label),
      };
    } catch (error) {
      console.warn("Failed to use advanced classification, using fallback");
    }
  }

  // ØªØµÙ†ÙŠÙ Ù…Ø¨Ø³Ø·
  return generateSimpleClassification(imageBitmap);
};

/**
 * ØªØµÙ†ÙŠÙ Ù…Ø¨Ø³Ø· Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ù„ÙˆØ§Ù† ÙˆØ§Ù„Ø£Ù†Ù…Ø§Ø·
 */
const generateSimpleClassification = (imageBitmap: ImageBitmap) => {
  const canvas = new OffscreenCanvas(32, 32);
  const ctx = canvas.getContext("2d")!;
  ctx.drawImage(imageBitmap, 0, 0, 32, 32);

  const imageData = ctx.getImageData(0, 0, 32, 32);
  let totalR = 0,
    totalG = 0,
    totalB = 0;
  const pixels = imageData.data.length / 4;

  for (let i = 0; i < imageData.data.length; i += 4) {
    totalR += imageData.data[i];
    totalG += imageData.data[i + 1];
    totalB += imageData.data[i + 2];
  }

  const avgR = totalR / pixels;
  const avgG = totalG / pixels;
  const avgB = totalB / pixels;

  // ØªØµÙ†ÙŠÙ Ø¨Ø³ÙŠØ· Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù„ÙˆØ§Ù† Ø§Ù„Ø³Ø§Ø¦Ø¯Ø©
  let classification = "Ø¹Ø§Ù…";
  const tags = [];

  if (avgG > avgR && avgG > avgB) {
    classification = "Ø·Ø¨ÙŠØ¹Ø©";
    tags.push("Ø£Ø®Ø¶Ø±", "Ø·Ø¨ÙŠØ¹Ø©", "Ù†Ø¨Ø§ØªØ§Øª");
  } else if (avgB > avgR && avgB > avgG) {
    classification = "Ø³Ù…Ø§Ø¡ Ø£Ùˆ Ù…Ø§Ø¡";
    tags.push("Ø£Ø²Ø±Ù‚", "Ø³Ù…Ø§Ø¡", "Ù…Ø§Ø¡");
  } else if (avgR > 150) {
    classification = "Ø¯Ø§ÙØ¦";
    tags.push("Ø£Ø­Ù…Ø±", "Ø¯Ø§ÙØ¦", "ØºØ±ÙˆØ¨");
  } else {
    tags.push("Ù…ØªÙ†ÙˆØ¹", "Ù…Ù„ÙˆÙ†", "Ø¹Ø§Ù…");
  }

  const descriptions = [
    "ØµÙˆØ±Ø© Ø¬Ù…ÙŠÙ„Ø© Ø¨Ø£Ù„ÙˆØ§Ù† Ø²Ø§Ù‡ÙŠØ©",
    "Ù„Ù‚Ø·Ø© Ø±Ø§Ø¦Ø¹Ø© ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ØªÙØ§ØµÙŠÙ„ Ù…Ø«ÙŠØ±Ø©",
    "ØªØµÙˆÙŠØ± Ø§Ø­ØªØ±Ø§ÙÙŠ Ø¨ØªØ±ÙƒÙŠØ¨ Ù…Ù…ØªØ§Ø²",
    "Ù…Ù†Ø¸Ø± Ø®Ù„Ø§Ø¨ ÙŠØ£Ø³Ø± Ø§Ù„Ø£Ù†Ø¸Ø§Ø±",
    "Ù„Ø­Ø¸Ø© Ù…Ù…ÙŠØ²Ø© ØªÙ… ØªÙˆØ«ÙŠÙ‚Ù‡Ø§ Ø¨Ø¹Ù†Ø§ÙŠØ©",
  ];

  return {
    classification,
    description: descriptions[Math.floor(Math.random() * descriptions.length)],
    tags,
  };
};

// Ø¨Ø¯Ø¡ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙÙˆØ±Ø§Ù‹ Ø¹Ù†Ø¯ ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¹Ø§Ù…Ù„.
initializeModels();

// Ø§Ù„Ø§Ø³ØªÙ…Ø§Ø¹ Ù„Ù„Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„ÙˆØ§Ø±Ø¯Ø© Ù…Ù† Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©.
// ÙƒÙ„ Ø±Ø³Ø§Ù„Ø© Ù‡ÙŠ Ø·Ù„Ø¨ Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© ØµÙˆØ±Ø©.
self.onmessage = async (event: MessageEvent<WorkerMessage>) => {
  const { id, type, imageBitmap } = event.data;

  // Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø§ÙƒØªÙ…Ø§Ù„ Ø§Ù„ØªÙ‡ÙŠØ¦Ø©
  if (!isInitialized) {
    self.postMessage({
      id,
      type: "ERROR",
      status: "error",
      error: "AI models still initializing, please wait...",
    } as WorkerResponse);
    return;
  }

  if (!imageBitmap) {
    self.postMessage({
      id,
      type: "ERROR",
      status: "error",
      error: "No image data provided",
    } as WorkerResponse);
    return;
  }

  try {
    // Ø¥Ø±Ø³Ø§Ù„ ØªØ­Ø¯ÙŠØ« Ø§Ù„ØªÙ‚Ø¯Ù…
    self.postMessage({
      id,
      type: "PROCESSING_COMPLETE",
      status: "progress",
      progress: 10,
    } as WorkerResponse);

    // Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠØ©
    const embeddings = await extractEmbeddings(imageBitmap);

    self.postMessage({
      id,
      type: "PROCESSING_COMPLETE",
      status: "progress",
      progress: 50,
    } as WorkerResponse);

    // ØªØµÙ†ÙŠÙ ÙˆÙˆØµÙ Ø§Ù„ØµÙˆØ±Ø©
    const { classification, description, tags } =
      await classifyAndDescribe(imageBitmap);

    self.postMessage({
      id,
      type: "PROCESSING_COMPLETE",
      status: "progress",
      progress: 90,
    } as WorkerResponse);

    // Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
    self.postMessage({
      id,
      type: "PROCESSING_COMPLETE",
      status: "completed",
      embeddings: new Float32Array(embeddings),
      classification,
      description,
      tags,
      progress: 100,
    } as WorkerResponse);
  } catch (e: any) {
    console.error(`ğŸ”´ AI Worker: Error processing image ${id}:`, e);
    self.postMessage({
      id,
      type: "ERROR",
      status: "error",
      error: e.message,
    } as WorkerResponse);
  }
};

// Ù…Ø¹Ø§Ù„Ø¬Ø© Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ø¹Ø§Ù…Ù„
self.onerror = (error) => {
  console.error("ğŸ”´ AI Worker: Critical error:", error);
};

console.log("ğŸš€ Knoux XÂ² AI Worker initialized and ready!");
